{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahim77890/machine-learning/blob/main/cold_start_recommendation_self_paced_semi_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ENKcP_y_dk5K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score, ndcg_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Download and extract the Amazon Reviews dataset\n",
        "def download_and_extract_dataset():\n",
        "    print(\"Downloading Amazon Reviews dataset...\")\n",
        "    path = kagglehub.dataset_download(\"kritanjalijain/amazon-reviews\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # Find and extract zip files if needed\n",
        "    zip_files = [f for f in os.listdir(path) if f.endswith('.zip')]\n",
        "\n",
        "    if zip_files:\n",
        "        for zip_file in zip_files:\n",
        "            zip_path = os.path.join(path, zip_file)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                print(f\"Extracting {zip_file}...\")\n",
        "                z.extractall(path)\n",
        "\n",
        "    return path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "N99Q3VeML1cT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from typing import Tuple, Optional\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def load_data(\n",
        "    dataset_path: str,\n",
        "    n_users: int = 1000,\n",
        "    fake_seed: int = 42,\n",
        "    text_feat_max: int = 100,\n",
        "    text_svd_components: int = 20,\n",
        "    max_reviews: int = 100000\n",
        ") -> Tuple[pd.DataFrame, dict, dict, Optional[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Load Amazon reviews (JSON or CSV), fake user IDs up front to avoid warnings,\n",
        "    add extra features, and extract optional TF-IDF + SVD text features.\n",
        "\n",
        "    Returns:\n",
        "      df: DataFrame with columns ['user_id','item_id','rating','text_length','title_length',...]\n",
        "      user_id_map: original user_id → index\n",
        "      item_id_map: original item_id → index\n",
        "      text_features: array of shape (n_samples, text_svd_components) or None\n",
        "    \"\"\"\n",
        "    # 1) Load raw data\n",
        "    json_files = [f for f in os.listdir(dataset_path) if f.endswith('.json')]\n",
        "    csv_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
        "\n",
        "    if json_files:\n",
        "        file_path = os.path.join(dataset_path, json_files[0])\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= max_reviews:\n",
        "                    break\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "        df = pd.DataFrame(data)\n",
        "    elif csv_files:\n",
        "        file_path = os.path.join(dataset_path, csv_files[0])\n",
        "        df = pd.read_csv(file_path, nrows=max_reviews)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No JSON or CSV files found in the dataset path\")\n",
        "\n",
        "    # 2) Immediately generate fake user_ids to ensure 'user_id' column exists\n",
        "    Faker.seed(fake_seed)\n",
        "    fake = Faker()\n",
        "    unique_fake_ids = [fake.uuid4() for _ in range(n_users)]\n",
        "    df['user_id'] = [random.choice(unique_fake_ids) for _ in range(len(df))]\n",
        "    logging.info(f\"Pre-assigned {n_users} fake users across {len(df)} records; 'user_id' column now present.\")\n",
        "\n",
        "    # 3) Identify core columns (now 'user_id' is guaranteed)\n",
        "    user_col = 'user_id'\n",
        "    # item detection\n",
        "    if 'asin' in df.columns:\n",
        "        item_col = 'asin'\n",
        "    elif 'product_id' in df.columns:\n",
        "        item_col = 'product_id'\n",
        "    else:\n",
        "        item_col = df.columns[1]\n",
        "    # rating detection\n",
        "    if 'overall' in df.columns:\n",
        "        rating_col = 'overall'\n",
        "    elif 'rating' in df.columns:\n",
        "        rating_col = 'rating'\n",
        "    else:\n",
        "        rating_col = None\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].min() >= 0 and df[col].max() <= 5:\n",
        "                rating_col = col\n",
        "                break\n",
        "    # text & title\n",
        "    text_col = 'reviewText' if 'reviewText' in df.columns else 'text' if 'text' in df.columns else None\n",
        "    title_col = 'summary' if 'summary' in df.columns else 'title' if 'title' in df.columns else None\n",
        "\n",
        "    # 4) Keep and rename necessary columns\n",
        "    cols = [c for c in [user_col, item_col, rating_col, text_col, title_col] if c]\n",
        "    df = df[cols].copy()\n",
        "    mapping = {user_col: 'user_id', item_col: 'item_id'}\n",
        "    if rating_col:\n",
        "        mapping[rating_col] = 'rating'\n",
        "    df.rename(columns=mapping, inplace=True)\n",
        "\n",
        "    # 5) Drop missing critical values\n",
        "    df.dropna(subset=['user_id', 'item_id'], inplace=True)\n",
        "    if 'rating' in df.columns:\n",
        "        df.dropna(subset=['rating'], inplace=True)\n",
        "        df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "    # 6) Add extra numeric features\n",
        "    if text_col:\n",
        "        df['text_length'] = df[text_col].fillna('').astype(str).apply(len)\n",
        "    if title_col:\n",
        "        df['title_length'] = df[title_col].fillna('').astype(str).apply(len)\n",
        "\n",
        "    # 7) Create index maps\n",
        "    user_ids = df['user_id'].unique()\n",
        "    item_ids = df['item_id'].unique()\n",
        "    user_id_map = {u: i for i, u in enumerate(user_ids)}\n",
        "    item_id_map = {v: j for j, v in enumerate(item_ids)}\n",
        "    df['user_idx'] = df['user_id'].map(user_id_map)\n",
        "    df['item_idx'] = df['item_id'].map(item_id_map)\n",
        "\n",
        "    # Save user_id_map\n",
        "    with open(\"user_id_map.pkl\", \"wb\") as f:\n",
        "        pickle.dump(user_id_map, f)\n",
        "\n",
        "    # Save item_id_map\n",
        "    with open(\"item_id_map.pkl\", \"wb\") as f:\n",
        "        pickle.dump(item_id_map, f)\n",
        "    \n",
        "\n",
        "    print(\"Mappings saved to user_id_map.pkl and item_id_map.pkl\")\n",
        "\n",
        "    # 8) Extract text features if possible\n",
        "    text_features = None\n",
        "    if text_col:\n",
        "        vec = TfidfVectorizer(max_features=text_feat_max, stop_words='english')\n",
        "        sample = df[text_col].fillna('').astype(str)\n",
        "        tfidf = vec.fit_transform(sample)\n",
        "        svd = TruncatedSVD(n_components=text_svd_components, random_state=fake_seed)\n",
        "        text_features = svd.fit_transform(tfidf)\n",
        "        logging.info(f\"Extracted text_features shape={text_features.shape}\")\n",
        "\n",
        "    return df, user_id_map, item_id_map, text_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xRA1JNGoSZQM"
      },
      "outputs": [],
      "source": [
        "# Create cold start scenario\n",
        "def create_cold_start_scenario(df, cold_start_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create a cold start scenario by hiding interactions for some users\n",
        "    \"\"\"\n",
        "    # Identify users with few interactions (naturally cold)\n",
        "    user_counts = df['user_idx'].value_counts()\n",
        "\n",
        "    # Find users with at least 5 interactions\n",
        "    qualified_users = user_counts[user_counts >= 10].index.tolist()\n",
        "\n",
        "    if len(qualified_users) == 0:\n",
        "        print(\"Warning: No users with enough interactions for cold start simulation\")\n",
        "        # Fall back to random selection\n",
        "        all_users = df['user_idx'].unique()\n",
        "        cold_users = np.random.choice(\n",
        "            all_users,\n",
        "            size=int(len(all_users) * cold_start_ratio),\n",
        "            replace=False\n",
        "        )\n",
        "    else:\n",
        "        # Select a subset of users to simulate as cold start\n",
        "        cold_users = np.random.choice(\n",
        "            qualified_users,\n",
        "            size=int(len(qualified_users) * cold_start_ratio),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "    print(f\"Selected {len(cold_users)} users as cold start users\")\n",
        "\n",
        "    # For cold start users, keep only 1-2 interactions as \"observed\" and move the rest to \"test\"\n",
        "    train_df = pd.DataFrame()\n",
        "    test_df = pd.DataFrame()\n",
        "\n",
        "    for user_idx in df['user_idx'].unique():\n",
        "        user_data = df[df['user_idx'] == user_idx].copy()\n",
        "\n",
        "        if user_idx in cold_users:\n",
        "            # For cold users, keep only 1-2 interactions as observed\n",
        "            n_observed = min(2, len(user_data))\n",
        "            observed = user_data.sample(n_observed)\n",
        "            holdout = user_data.drop(observed.index)\n",
        "\n",
        "            train_df = pd.concat([train_df, observed])\n",
        "            test_df = pd.concat([test_df, holdout])\n",
        "        else:\n",
        "            # For warm users, use 90/10 split\n",
        "            user_train, user_test = train_test_split(user_data, test_size=0.1, random_state=42)\n",
        "            train_df = pd.concat([train_df, user_train])\n",
        "            test_df = pd.concat([test_df, user_test])\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} interactions\")\n",
        "    print(f\"Test set: {len(test_df)} interactions\")\n",
        "\n",
        "    # Create a mask to identify cold users\n",
        "    cold_user_mask = np.isin(df['user_idx'].unique(), cold_users)\n",
        "\n",
        "    return train_df, test_df, cold_user_mask, cold_users\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "hlkdSe6BSf3J"
      },
      "outputs": [],
      "source": [
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20, bias=True):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        \n",
        "        # User and item embeddings\n",
        "        self.user_embeddings = nn.Embedding(n_users, n_factors)\n",
        "        self.item_embeddings = nn.Embedding(n_items, n_factors)\n",
        "        \n",
        "        # Initialize embeddings with small random values\n",
        "        nn.init.normal_(self.user_embeddings.weight, mean=0, std=0.01)\n",
        "        nn.init.normal_(self.item_embeddings.weight, mean=0, std=0.01)\n",
        "        \n",
        "        self.bias = bias\n",
        "        \n",
        "        if bias:\n",
        "            # Global bias, user bias, and item bias\n",
        "            self.global_bias = nn.Parameter(torch.zeros(1))\n",
        "            self.user_bias = nn.Embedding(n_users, 1)\n",
        "            self.item_bias = nn.Embedding(n_items, 1)\n",
        "            \n",
        "            # Initialize bias terms\n",
        "            nn.init.zeros_(self.user_bias.weight)\n",
        "            nn.init.zeros_(self.item_bias.weight)\n",
        "    \n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Get embeddings\n",
        "        user_vectors = self.user_embeddings(user_indices)\n",
        "        item_vectors = self.item_embeddings(item_indices)\n",
        "        \n",
        "        # Element-wise product of user and item vectors\n",
        "        interaction = torch.sum(user_vectors * item_vectors, dim=1, keepdim=True)\n",
        "        \n",
        "        if self.bias:\n",
        "            # Add bias terms\n",
        "            user_b = self.user_bias(user_indices)\n",
        "            item_b = self.item_bias(item_indices)\n",
        "            \n",
        "            # Predicted rating\n",
        "            rating = self.global_bias + user_b + item_b + interaction\n",
        "        else:\n",
        "            rating = interaction\n",
        "        \n",
        "        # Return predicted rating (flatten from 2D to 1D)\n",
        "        return rating.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "hhi3YwSTUq4e"
      },
      "outputs": [],
      "source": [
        "class SelfPacedMatrixFactorization:\n",
        "    def __init__(self, n_users, n_items, n_factors=20, lambda_start=0.1, \n",
        "                 lambda_end=5.0, lambda_steps=3, learning_rate=0.01, \n",
        "                 weight_decay=0.001, batch_size=256, device='cpu'):\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "        self.lambda_start = lambda_start\n",
        "        self.lambda_end = lambda_end\n",
        "        self.lambda_steps = lambda_steps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize model\n",
        "        self.model = MatrixFactorization(n_users, n_items, n_factors, bias=True)\n",
        "        self.model.to(device)\n",
        "        \n",
        "        # Initialize optimizer\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(), \n",
        "            lr=learning_rate, \n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        \n",
        "        # Loss function\n",
        "        self.criterion = nn.MSELoss(reduction='none')  # We'll handle reduction manually for sample weights\n",
        "        \n",
        "    def fit(self, train_df, val_df=None, epochs=5):\n",
        "        \"\"\"\n",
        "        Train the model using self-paced learning\n",
        "        \"\"\"\n",
        "        # Add sample weight column if it doesn't exist\n",
        "        if 'sample_weight' not in train_df.columns:\n",
        "            train_df['sample_weight'] = 1.0\n",
        "            \n",
        "        # Add is_pseudo column if it doesn't exist\n",
        "        if 'is_pseudo' not in train_df.columns:\n",
        "            train_df['is_pseudo'] = 0\n",
        "            \n",
        "        # Calculate lambda schedule\n",
        "        lambda_values = np.exp(np.linspace(\n",
        "            np.log(self.lambda_start),\n",
        "            np.log(self.lambda_end),\n",
        "            self.lambda_steps\n",
        "        ))\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Update lambda value based on current epoch\n",
        "            if self.lambda_steps > 1:\n",
        "                lambda_idx = min(epoch // (epochs // self.lambda_steps), self.lambda_steps - 1)\n",
        "                lambda_value = lambda_values[lambda_idx]\n",
        "            else:\n",
        "                lambda_value = self.lambda_start\n",
        "                \n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Lambda: {lambda_value:.4f}\")\n",
        "            \n",
        "            # Create DataLoader\n",
        "            train_dataset = RecommendationDataset(train_df)\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset, \n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True\n",
        "            )\n",
        "            \n",
        "            # Train one epoch\n",
        "            self._train_epoch(train_loader, lambda_value)\n",
        "            \n",
        "            # Evaluate on validation set if provided\n",
        "            if val_df is not None:\n",
        "                rmse, mae = self.evaluate(val_df)\n",
        "                print(f\"Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "                \n",
        "    def _train_epoch(self, data_loader, lambda_value):\n",
        "        \"\"\"\n",
        "        Train for one epoch with self-paced learning\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch in data_loader:\n",
        "            user_indices = batch['user_idx'].to(self.device)\n",
        "            item_indices = batch['item_idx'].to(self.device)\n",
        "            ratings = batch['rating'].to(self.device)\n",
        "            is_pseudo = batch['is_pseudo'].to(self.device) if 'is_pseudo' in batch else torch.zeros_like(ratings)\n",
        "            \n",
        "            # Get current sample weights\n",
        "            if 'sample_weight' in batch:\n",
        "                sample_weights = batch['sample_weight'].to(self.device)\n",
        "            else:\n",
        "                sample_weights = torch.ones_like(ratings)\n",
        "                \n",
        "            # Forward pass\n",
        "            outputs = self.model(user_indices, item_indices)\n",
        "            \n",
        "            # Calculate loss with sample weights\n",
        "            loss_values = self.criterion(outputs, ratings)\n",
        "            \n",
        "            # Update sample weights for pseudo-labeled examples\n",
        "            with torch.no_grad():\n",
        "                # Only update weights for pseudo-labeled examples\n",
        "                pseudo_mask = (is_pseudo > 0)\n",
        "                if pseudo_mask.sum() > 0:\n",
        "                    v_values = torch.exp(-loss_values[pseudo_mask] / lambda_value)\n",
        "                    sample_weights[pseudo_mask] = v_values\n",
        "            \n",
        "            # Apply sample weights to loss\n",
        "            weighted_loss = (loss_values * sample_weights).mean()\n",
        "            \n",
        "            # Backward pass and optimization\n",
        "            self.optimizer.zero_grad()\n",
        "            weighted_loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_loss += weighted_loss.item()\n",
        "            \n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"  Training loss: {avg_loss:.4f}\")\n",
        "        \n",
        "    def predict(self, user_indices, item_indices):\n",
        "        \"\"\"\n",
        "        Predict ratings for given user-item pairs\n",
        "        \"\"\"\n",
        "        # Convert to numpy arrays if needed\n",
        "        user_indices = np.asarray(user_indices)\n",
        "        item_indices = np.asarray(item_indices)\n",
        "        \n",
        "        # Move to device\n",
        "        user_indices_tensor = torch.LongTensor(user_indices).to(self.device)\n",
        "        item_indices_tensor = torch.LongTensor(item_indices).to(self.device)\n",
        "        \n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Predict ratings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(user_indices_tensor, item_indices_tensor)\n",
        "            \n",
        "        # Convert to numpy array\n",
        "        predictions = outputs.cpu().numpy()\n",
        "        \n",
        "        # Handle any NaN values (convert to minimum rating)\n",
        "        predictions = np.nan_to_num(predictions, nan=1.0)\n",
        "        \n",
        "        # Clip predictions to valid rating range (e.g., 1-5)\n",
        "        predictions = np.clip(predictions, 1.0, 5.0)\n",
        "        \n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "zOuF9trdUxpg"
      },
      "outputs": [],
      "source": [
        "# Semi-supervised learning with pseudo-labeling\n",
        "class SemiSupervisedRecommender:\n",
        "    def __init__(self, base_model, n_users, n_items, confidence_threshold=0.7):\n",
        "        self.base_model = base_model\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "\n",
        "        # For clustering users\n",
        "        self.user_clusters = None\n",
        "        self.item_clusters = None\n",
        "\n",
        "    def cluster_users(self, train_df, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Cluster users based on their item interactions\n",
        "        \"\"\"\n",
        "        # Create user-item interaction matrix\n",
        "        user_item_matrix = np.zeros((self.n_users, self.n_items))\n",
        "\n",
        "        for _, row in train_df.iterrows():\n",
        "            user_idx = row['user_idx']\n",
        "            item_idx = row['item_idx']\n",
        "            rating = row['rating']\n",
        "            user_item_matrix[user_idx, item_idx] = rating\n",
        "\n",
        "        # Normalize rows (users)\n",
        "        row_sums = user_item_matrix.sum(axis=1)\n",
        "        mask = row_sums > 0\n",
        "        user_item_matrix[mask] = user_item_matrix[mask] / row_sums[mask, np.newaxis]\n",
        "\n",
        "        # Cluster users\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        self.user_clusters = kmeans.fit_predict(user_item_matrix)\n",
        "\n",
        "        # Also cluster items by their user interactions (transpose)\n",
        "        item_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        self.item_clusters = item_kmeans.fit_predict(user_item_matrix.T)\n",
        "\n",
        "        return self.user_clusters, self.item_clusters\n",
        "\n",
        "    def generate_pseudo_labels(self, train_df, cold_users, top_k=20, bottom_k=10):\n",
        "        \"\"\"\n",
        "        Generate pseudo labels for cold start users based on similar users\n",
        "        \"\"\"\n",
        "        if self.user_clusters is None:\n",
        "            self.cluster_users(train_df)\n",
        "\n",
        "        # Create user-item matrix for existing ratings\n",
        "        user_item_matrix = np.zeros((self.n_users, self.n_items))\n",
        "        for _, row in train_df.iterrows():\n",
        "            user_idx = row['user_idx']\n",
        "            item_idx = row['item_idx']\n",
        "            rating = row['rating']\n",
        "            user_item_matrix[user_idx, item_idx] = rating\n",
        "\n",
        "        # Get average item ratings per cluster\n",
        "        cluster_item_avg = np.zeros((np.max(self.user_clusters) + 1, self.n_items))\n",
        "        cluster_item_count = np.zeros((np.max(self.user_clusters) + 1, self.n_items))\n",
        "\n",
        "        for user_idx in range(self.n_users):\n",
        "            if user_idx in cold_users:\n",
        "                continue\n",
        "\n",
        "            cluster_id = self.user_clusters[user_idx]\n",
        "            for item_idx in range(self.n_items):\n",
        "                rating = user_item_matrix[user_idx, item_idx]\n",
        "                if rating > 0:\n",
        "                    cluster_item_avg[cluster_id, item_idx] += rating\n",
        "                    cluster_item_count[cluster_id, item_idx] += 1\n",
        "\n",
        "        # Calculate average (avoid division by zero)\n",
        "        cluster_item_count[cluster_item_count == 0] = 1\n",
        "        cluster_item_avg = cluster_item_avg / cluster_item_count\n",
        "\n",
        "        # Generate pseudo labels for cold users\n",
        "        pseudo_labeled_data = []\n",
        "\n",
        "        for user_idx in cold_users:\n",
        "            # Get user's cluster based on their limited interactions\n",
        "            user_cluster = self.user_clusters[user_idx]\n",
        "\n",
        "            # Get cluster's average ratings\n",
        "            cluster_avg_ratings = cluster_item_avg[user_cluster]\n",
        "\n",
        "            # Find items with highest and lowest ratings in the cluster\n",
        "            # (excluding items the user has already rated)\n",
        "            rated_items = set(train_df[train_df['user_idx'] == user_idx]['item_idx'])\n",
        "            unrated_items = np.array([i for i in range(self.n_items) if i not in rated_items])\n",
        "\n",
        "            if len(unrated_items) == 0:\n",
        "                continue\n",
        "\n",
        "            # Get cluster ratings for unrated items\n",
        "            unrated_ratings = cluster_avg_ratings[unrated_items]\n",
        "\n",
        "            # Select top-k and bottom-k items\n",
        "            top_indices = unrated_items[np.argsort(-unrated_ratings)[:top_k]]\n",
        "            bottom_indices = unrated_items[np.argsort(unrated_ratings)[:bottom_k]]\n",
        "\n",
        "            # Create pseudo labels (high rating for top items, low for bottom)\n",
        "            for item_idx in top_indices:\n",
        "                if cluster_avg_ratings[item_idx] > 0:\n",
        "                    pseudo_labeled_data.append({\n",
        "                        'user_idx': user_idx,\n",
        "                        'item_idx': item_idx,\n",
        "                        'rating': min(5.0, cluster_avg_ratings[item_idx] * 1.1),  # Slightly higher than cluster avg\n",
        "                        'is_pseudo': 1\n",
        "                    })\n",
        "\n",
        "            for item_idx in bottom_indices:\n",
        "                if cluster_avg_ratings[item_idx] > 0:\n",
        "                    pseudo_labeled_data.append({\n",
        "                        'user_idx': user_idx,\n",
        "                        'item_idx': item_idx,\n",
        "                        'rating': max(1.0, cluster_avg_ratings[item_idx] * 0.9),  # Slightly lower than cluster avg\n",
        "                        'is_pseudo': 1\n",
        "                    })\n",
        "\n",
        "        pseudo_df = pd.DataFrame(pseudo_labeled_data)\n",
        "        print(f\"Generated {len(pseudo_df)} pseudo labels for {len(cold_users)} cold start users\")\n",
        "\n",
        "        return pseudo_df\n",
        "\n",
        "    def train_semi_supervised(self, train_df, test_df, cold_users,\n",
        "                              epochs_per_stage=5, total_stages=3):\n",
        "        \"\"\"\n",
        "        Train model using semi-supervised approach with self-paced learning\n",
        "        \"\"\"\n",
        "        # First train the base model on labeled data\n",
        "        print(\"\\nStage 1: Training on labeled data only\")\n",
        "        self.base_model.fit(train_df, val_df=None, epochs=epochs_per_stage)\n",
        "\n",
        "        # Initialize training with pseudo-labels\n",
        "        augmented_train_df = train_df.copy()\n",
        "\n",
        "        for stage in range(2, total_stages + 1):\n",
        "            print(f\"\\nStage {stage}: Generating pseudo-labels and retraining\")\n",
        "\n",
        "            # Generate pseudo-labels\n",
        "            pseudo_df = self.generate_pseudo_labels(\n",
        "                augmented_train_df,\n",
        "                cold_users,\n",
        "                top_k=5 * stage,  # Increase number of pseudo-labels with each stage\n",
        "                bottom_k=3 * stage\n",
        "            )\n",
        "\n",
        "            if len(pseudo_df) == 0:\n",
        "                print(\"No pseudo-labels generated, skipping stage\")\n",
        "                continue\n",
        "\n",
        "            # Add pseudo-labels to training data\n",
        "            augmented_train_df = pd.concat([train_df, pseudo_df])\n",
        "\n",
        "            # Make sure pseudo-labels have a weight column (for self-paced learning)\n",
        "            if 'is_pseudo' not in augmented_train_df:\n",
        "                augmented_train_df['is_pseudo'] = 0\n",
        "\n",
        "            # Train with augmented data\n",
        "            self.base_model.fit(augmented_train_df, val_df=None, epochs=epochs_per_stage)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            rmse, mae = self.evaluate(test_df)\n",
        "            print(f\"Stage {stage} Test RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "            # Evaluate specifically for cold users\n",
        "            cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "            if len(cold_test_df) > 0:\n",
        "                cold_rmse, cold_mae = self.evaluate(cold_test_df)\n",
        "                print(f\"Stage {stage} Cold Users Test RMSE: {cold_rmse:.4f}, MAE: {cold_mae:.4f}\")\n",
        "\n",
        "    def evaluate(self, test_df):\n",
        "        \"\"\"\n",
        "        Evaluate model on test data\n",
        "        \"\"\"\n",
        "        user_indices = test_df['user_idx'].values\n",
        "        item_indices = test_df['item_idx'].values\n",
        "        true_ratings = test_df['rating'].values\n",
        "\n",
        "        # Get predictions\n",
        "        pred_ratings = self.base_model.predict(user_indices, item_indices)\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings))\n",
        "        mae = mean_absolute_error(true_ratings, pred_ratings)\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    def get_recommendations(self, user_idx, n_recommendations=10, exclude_rated=True, rated_items=None):\n",
        "    \n",
        "    # Get all items\n",
        "        all_items = np.arange(self.n_items)\n",
        "    \n",
        "    # If excluding rated items, filter them out\n",
        "        if exclude_rated:\n",
        "            if rated_items is None:\n",
        "            # You would need to pass the original training dataframe here\n",
        "            # For now, assume rated_items is provided or don't exclude\n",
        "                print(\"Warning: rated_items not provided, not excluding already rated items\")\n",
        "                rated_items = set()\n",
        "        \n",
        "        # Filter out rated items\n",
        "            unrated_items = np.array([i for i in all_items if i not in rated_items])\n",
        "            items_to_predict = unrated_items\n",
        "        else:\n",
        "            items_to_predict = all_items\n",
        "    \n",
        "    # If no items to predict, return empty list\n",
        "        if len(items_to_predict) == 0:\n",
        "            return []\n",
        "    \n",
        "    # Predict ratings for all items\n",
        "        user_indices = np.full_like(items_to_predict, user_idx)\n",
        "        predicted_ratings = self.base_model.predict(user_indices, items_to_predict)\n",
        "    \n",
        "    # Get top N items by predicted rating\n",
        "        top_indices = np.argsort(-predicted_ratings)[:n_recommendations]\n",
        "        top_items = items_to_predict[top_indices]\n",
        "        top_ratings = predicted_ratings[top_indices]\n",
        "    \n",
        "    # Debug: Print range of predicted ratings\n",
        "        min_rating = np.min(predicted_ratings)\n",
        "        max_rating = np.max(predicted_ratings)\n",
        "        mean_rating = np.mean(predicted_ratings)\n",
        "        print(f\"Debug - Rating range: min={min_rating:.4f}, max={max_rating:.4f}, mean={mean_rating:.4f}\")\n",
        "    \n",
        "        return list(zip(top_items, top_ratings))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "wPmwO6fcU43O"
      },
      "outputs": [],
      "source": [
        "# Evaluation utilities\n",
        "def calculate_precision_recall_at_k(predictions, test_df, k=10):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K and Recall@K\n",
        "    \"\"\"\n",
        "    user_true_items = {}\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_idx = row['user_idx']\n",
        "        item_idx = row['item_idx']\n",
        "        rating = row['rating']\n",
        "\n",
        "        if rating >= 4.0:  # Consider as relevant if rating >= 4\n",
        "            if user_idx not in user_true_items:\n",
        "                user_true_items[user_idx] = set()\n",
        "            user_true_items[user_idx].add(item_idx)\n",
        "\n",
        "    precision_sum = 0\n",
        "    recall_sum = 0\n",
        "    user_count = 0\n",
        "\n",
        "    for user_idx, user_predictions in predictions.items():\n",
        "        if user_idx not in user_true_items:\n",
        "            continue\n",
        "\n",
        "        # Get top k predictions\n",
        "        recommended_items = [item for item, _ in user_predictions[:k]]\n",
        "        relevant_items = user_true_items[user_idx]\n",
        "\n",
        "        if not relevant_items:\n",
        "            continue\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        n_relevant_recommended = len(set(recommended_items) & relevant_items)\n",
        "        precision = n_relevant_recommended / min(k, len(recommended_items)) if recommended_items else 0\n",
        "        recall = n_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
        "\n",
        "        precision_sum += precision\n",
        "        recall_sum += recall\n",
        "        user_count += 1\n",
        "\n",
        "    # Calculate average\n",
        "    avg_precision = precision_sum / user_count if user_count > 0 else 0\n",
        "    avg_recall = recall_sum / user_count if user_count > 0 else 0\n",
        "\n",
        "    return avg_precision, avg_recall\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "cIaGLQmPU9pj"
      },
      "outputs": [],
      "source": [
        "def evaluate_ndcg_at_k(predictions, test_df, k=10):\n",
        "    \"\"\"\n",
        "    Calculate NDCG@K\n",
        "    \"\"\"\n",
        "    # Create user-item-rating dictionary from test data\n",
        "    user_item_ratings = {}\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_idx = row['user_idx']\n",
        "        item_idx = row['item_idx']\n",
        "        rating = row['rating']\n",
        "\n",
        "        if user_idx not in user_item_ratings:\n",
        "            user_item_ratings[user_idx] = {}\n",
        "        user_item_ratings[user_idx][item_idx] = rating\n",
        "\n",
        "    ndcg_sum = 0\n",
        "    user_count = 0\n",
        "\n",
        "    for user_idx, user_predictions in predictions.items():\n",
        "        if user_idx not in user_item_ratings:\n",
        "            continue\n",
        "\n",
        "        # Get predicted items and their scores\n",
        "        pred_items = []\n",
        "        pred_scores = []\n",
        "        for item_idx, score in user_predictions[:k]:\n",
        "            pred_items.append(item_idx)\n",
        "            pred_scores.append(score)\n",
        "\n",
        "        # Get true ratings for predicted items (0 if not rated)\n",
        "        true_scores = []\n",
        "        for item_idx in pred_items:\n",
        "            true_scores.append(user_item_ratings[user_idx].get(item_idx, 0))\n",
        "\n",
        "        # Calculate NDCG\n",
        "        if sum(true_scores) > 0:  # Skip if no relevant items\n",
        "            ndcg = ndcg_score([true_scores], [pred_scores])\n",
        "            ndcg_sum += ndcg\n",
        "            user_count += 1\n",
        "\n",
        "    # Calculate average NDCG\n",
        "    avg_ndcg = ndcg_sum / user_count if user_count > 0 else 0\n",
        "\n",
        "    return avg_ndcg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "b6co6TkBVBer"
      },
      "outputs": [],
      "source": [
        "def evaluate_time_to_quality(model, train_df, test_df, cold_users,\n",
        "                            epochs=[1, 5, 10, 20, 50], n_recommendations=10):\n",
        "    \"\"\"\n",
        "    Evaluate how quickly the model improves for cold start users\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Initial evaluation without training\n",
        "    cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "    rmse_0, mae_0 = 0, 0  # No predictions yet\n",
        "    results.append({\n",
        "        'epochs': 0,\n",
        "        'rmse': rmse_0,\n",
        "        'mae': mae_0\n",
        "    })\n",
        "\n",
        "    # Train and evaluate incrementally\n",
        "    cumulative_epochs = 0\n",
        "\n",
        "    for n_epochs in epochs:\n",
        "        # Train for additional epochs\n",
        "        model.base_model.fit(train_df, epochs=n_epochs, verbose=False)\n",
        "        cumulative_epochs += n_epochs\n",
        "\n",
        "        # Evaluate\n",
        "        rmse, mae = model.evaluate(cold_test_df)\n",
        "\n",
        "        results.append({\n",
        "            'epochs': cumulative_epochs,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bXk7V-_rVFnU",
        "outputId": "3c8ca2f7-c102-48a3-b740-9c67f4088600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting User Cold Start Recommendation System\n",
            "Downloading Amazon Reviews dataset...\n",
            "Dataset downloaded to: C:\\Users\\M Affan Amir\\.cache\\kagglehub\\datasets\\kritanjalijain\\amazon-reviews\\versions\\2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-11 22:16:13 INFO Pre-assigned 1000 fake users across 100000 records; 'user_id' column now present.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mappings saved to user_id_map.pkl and item_id_map.pkl\n",
            "Selected 200 users as cold start users\n",
            "Train set: 72128 interactions\n",
            "Test set: 27865 interactions\n",
            "\n",
            "Dataset Statistics:\n",
            "Total users: 1000\n",
            "Total items: 87639\n",
            "Total interactions: 99993\n",
            "Cold start users: 200 (20.0%)\n",
            "\n",
            "Initializing Self-Paced Matrix Factorization model...\n",
            "Using device: cpu\n",
            "\n",
            "Initializing Semi-supervised Recommender...\n",
            "\n",
            "Training semi-supervised model...\n",
            "\n",
            "Stage 1: Training on labeled data only\n",
            "Epoch 1/5, Lambda: 0.1000\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'RecommendationDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 5. Train semi-supervised model\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining semi-supervised model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43msemi_supervised\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_semi_supervised\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcold_users\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcold_users\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs_per_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_stages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m    103\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# 6. Evaluate model performance\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating model performance...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mSemiSupervisedRecommender.train_semi_supervised\u001b[39m\u001b[34m(self, train_df, test_df, cold_users, epochs_per_stage, total_stages)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# First train the base model on labeled data\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStage 1: Training on labeled data only\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs_per_stage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Initialize training with pseudo-labels\u001b[39;00m\n\u001b[32m    134\u001b[39m augmented_train_df = train_df.copy()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mSelfPacedMatrixFactorization.fit\u001b[39m\u001b[34m(self, train_df, val_df, epochs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Lambda: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m train_dataset = \u001b[43mRecommendationDataset\u001b[49m(train_df)\n\u001b[32m     62\u001b[39m train_loader = DataLoader(\n\u001b[32m     63\u001b[39m     train_dataset, \n\u001b[32m     64\u001b[39m     batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m     65\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'RecommendationDataset' is not defined"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "from faker import Faker  # For fake user_id generation\n",
        "import logging\n",
        "import random\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def parse_args(argv=None) -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Sentiment classification with fake user IDs and text features\"\n",
        "    )\n",
        "    parser.add_argument(\"--input\", required=True, help=\"Path to input CSV file with columns: polarity,title,text\")\n",
        "    parser.add_argument(\"--n_users\", type=int, default=1000, help=\"Number of unique fake users to simulate\")\n",
        "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"Test set fraction\")\n",
        "    parser.add_argument(\"--random_seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size for PyTorch DataLoader\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    return parser.parse_args(argv)\n",
        "\n",
        "def setup_logging():\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    )\n",
        "\n",
        "print(\"Starting User Cold Start Recommendation System\")\n",
        "\n",
        "# 1. Download and load the dataset\n",
        "dataset_path = download_and_extract_dataset()\n",
        "\n",
        "# Create an argument parser\n",
        "parser = argparse.ArgumentParser(description=\"User Cold Start Recommendation System\")\n",
        "parser.add_argument(\"--input\", required=True, help=\"Path to the input dataset\")\n",
        "parser.add_argument(\"--n_users\", type=int, default=500, help=\"Number of unique fake users to simulate\")\n",
        "parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "parser.add_argument(\"--random_seed\", type=int, default=3, help=\"seeder for randomization\")\n",
        "\n",
        "# Parse the arguments\n",
        "args = parser.parse_args([\n",
        "    \"--input\", dataset_path,\n",
        "    \"--n_users\", \"500\",\n",
        "    \"--epochs\", \"3\",\n",
        "    \"--random_seed\", \"42\"\n",
        "])\n",
        "setup_logging()\n",
        "np.random.seed(args.random_seed)\n",
        "random.seed(args.random_seed)\n",
        "torch.manual_seed(args.random_seed)\n",
        "\n",
        "df, user_id_map, item_id_map, text_features = load_data(dataset_path)\n",
        "\n",
        "# 2. Create cold start scenario\n",
        "train_df, test_df, cold_user_mask, cold_users = create_cold_start_scenario(df)\n",
        "\n",
        "# Print some statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Total users: {len(user_id_map)}\")\n",
        "print(f\"Total items: {len(item_id_map)}\")\n",
        "print(f\"Total interactions: {len(df)}\")\n",
        "print(f\"Cold start users: {sum(cold_user_mask)} ({sum(cold_user_mask)/len(user_id_map):.1%})\")\n",
        "\n",
        "# 3. Initialize models\n",
        "n_users = len(user_id_map)\n",
        "n_items = len(item_id_map)\n",
        "\n",
        "print(\"\\nInitializing Self-Paced Matrix Factorization model...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "spmf = SelfPacedMatrixFactorization(\n",
        "    n_users=n_users,\n",
        "    n_items=n_items,\n",
        "    n_factors=30,\n",
        "    lambda_start=0.1,\n",
        "    lambda_end=5.0,\n",
        "    lambda_steps=3,\n",
        "    learning_rate=0.01,\n",
        "    weight_decay=0.001,\n",
        "    batch_size=256,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 4. Initialize semi-supervised recommender\n",
        "print(\"\\nInitializing Semi-supervised Recommender...\")\n",
        "semi_supervised = SemiSupervisedRecommender(\n",
        "    base_model=spmf,\n",
        "    n_users=n_users,\n",
        "    n_items=n_items,\n",
        "    confidence_threshold=0.8\n",
        ")\n",
        "\n",
        "# 5. Train semi-supervised model\n",
        "print(\"\\nTraining semi-supervised model...\")\n",
        "semi_supervised.train_semi_supervised(\n",
        "    train_df=train_df,\n",
        "    test_df=test_df,\n",
        "    cold_users=cold_users,\n",
        "    epochs_per_stage=5,\n",
        "    total_stages=3\n",
        ")\n",
        "\n",
        "# 6. Evaluate model performance\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "\n",
        "# Overall performance\n",
        "overall_rmse, overall_mae = semi_supervised.evaluate(test_df)\n",
        "print(f\"Overall Test RMSE: {overall_rmse:.4f}, MAE: {overall_mae:.4f}\")\n",
        "\n",
        "# Performance on cold users\n",
        "cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "if len(cold_test_df) > 0:\n",
        "    cold_rmse, cold_mae = semi_supervised.evaluate(cold_test_df)\n",
        "    print(f\"Cold Users Test RMSE: {cold_rmse:.4f}, MAE: {cold_mae:.4f}\")\n",
        "\n",
        "# 7. Generate recommendations\n",
        "print(\"\\nGenerating recommendations for sample users...\")\n",
        "\n",
        "# Sample a few users for recommendation examples\n",
        "sample_users = np.random.choice(cold_users, min(5, len(cold_users)), replace=False)\n",
        "\n",
        "# Generate recommendations for a user\n",
        "print(\"\\nGenerating recommendations for sample users...\")\n",
        "\n",
        "# Sample a few users for recommendation examples\n",
        "sample_users = np.random.choice(cold_users, min(5, len(cold_users)), replace=False)\n",
        "\n",
        "user_recommendations = {}\n",
        "for user_idx in sample_users:\n",
        "    recommendations = semi_supervised.get_recommendations(user_idx, n_recommendations=10)\n",
        "    user_recommendations[user_idx] = recommendations\n",
        "\n",
        "    print(f\"\\nTop 10 recommendations for user {user_idx}:\")\n",
        "    for i, (item_idx, score) in enumerate(recommendations, 1):\n",
        "        # Directly return item_idx instead of mapping to item_id\n",
        "        print(f\"{i}. Item Index: {item_idx} - Score: {score:.8f}\")\n",
        "\n",
        "# 8. Analyze time-to-quality\n",
        "print(\"\\nAnalyzing time-to-quality for cold users...\")\n",
        "\n",
        "# Create a fresh model for this analysis\n",
        "fresh_spmf = SelfPacedMatrixFactorization(\n",
        "    n_users=n_users,\n",
        "    n_items=n_items,\n",
        "    n_factors=30,\n",
        "    lambda_start=0.1,\n",
        "    lambda_end=5.0,\n",
        "    lambda_steps=3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "fresh_semi = SemiSupervisedRecommender(\n",
        "    base_model=fresh_spmf,\n",
        "    n_users=n_users,\n",
        "    n_items=n_items\n",
        ")\n",
        "\n",
        "ttq_results = evaluate_time_to_quality(\n",
        "    model=fresh_semi,\n",
        "    train_df=train_df,\n",
        "    test_df=test_df,\n",
        "    cold_users=cold_users,\n",
        "    epochs=[1, 4, 5, 10]  # Smaller for demonstration\n",
        ")\n",
        "\n",
        "print(\"\\nTime-to-quality results:\")\n",
        "print(ttq_results)\n",
        "\n",
        "# 9. Log metrics and register the model in MLflow\n",
        "mlflow.set_experiment(\"Cold Start Recommendation System\")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Log model parameters\n",
        "    mlflow.log_param(\"n_users\", n_users)\n",
        "    mlflow.log_param(\"n_items\", n_items)\n",
        "    mlflow.log_param(\"n_factors\", 30)\n",
        "    mlflow.log_param(\"learning_rate\", 0.01)\n",
        "    mlflow.log_param(\"epochs\", 3)\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"overall_rmse\", overall_rmse)\n",
        "    mlflow.log_metric(\"overall_mae\", overall_mae)\n",
        "    if len(cold_test_df) > 0:\n",
        "        mlflow.log_metric(\"cold_rmse\", cold_rmse)\n",
        "        mlflow.log_metric(\"cold_mae\", cold_mae)\n",
        "\n",
        "    # Log time-to-quality results as an artifact (e.g., CSV file)\n",
        "    ttq_results.to_csv(\"time_to_quality.csv\", index=False)\n",
        "    mlflow.log_artifact(\"time_to_quality.csv\")\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.pytorch.log_model(spmf.model, \"model\")\n",
        "\n",
        "    # Register the model in MLflow Model Registry\n",
        "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/model\"\n",
        "    mlflow.register_model(model_uri=model_uri, name=\"ColdStartRecommendationModel\")\n",
        "\n",
        "print(\"\\nRecommendation system training, evaluation, and model registration complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying the first 5 rows of the dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>user_idx</th>\n",
              "      <th>item_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4f11d8dc-5cd3-4369-84aa-c1b75ca0c428</td>\n",
              "      <td>One of the best game music soundtracks - for a...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>82010c62-f5f5-4b22-8e8f-a8e0284d82e5</td>\n",
              "      <td>Batteries died within a year ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ab9099a4-35a2-40ae-9af3-05535ec42e08</td>\n",
              "      <td>works fine, but Maha Energy is better</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4758367b-a649-4cdc-907b-907265e58f34</td>\n",
              "      <td>Great for the non-audiophile</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>033d2bce-575a-4d2c-a5c5-650c8186a576</td>\n",
              "      <td>DVD Player crapped out after one year</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                user_id  \\\n",
              "0  4f11d8dc-5cd3-4369-84aa-c1b75ca0c428   \n",
              "1  82010c62-f5f5-4b22-8e8f-a8e0284d82e5   \n",
              "2  ab9099a4-35a2-40ae-9af3-05535ec42e08   \n",
              "3  4758367b-a649-4cdc-907b-907265e58f34   \n",
              "4  033d2bce-575a-4d2c-a5c5-650c8186a576   \n",
              "\n",
              "                                             item_id  rating  user_idx  \\\n",
              "0  One of the best game music soundtracks - for a...     2.0         0   \n",
              "1                   Batteries died within a year ...     1.0         1   \n",
              "2              works fine, but Maha Energy is better     2.0         2   \n",
              "3                       Great for the non-audiophile     2.0         3   \n",
              "4              DVD Player crapped out after one year     1.0         4   \n",
              "\n",
              "   item_idx  \n",
              "0         0  \n",
              "1         1  \n",
              "2         2  \n",
              "3         3  \n",
              "4         4  "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the head of the dataset\n",
        "print(\"Displaying the first 5 rows of the dataset:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPC9ddl8+BbZXf4uFWusEFy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
