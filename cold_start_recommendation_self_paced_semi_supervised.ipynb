{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ENKcP_y_dk5K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score, ndcg_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import kagglehub\n",
        "import zipfile\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Download and extract the Amazon Reviews dataset\n",
        "def download_and_extract_dataset():\n",
        "    print(\"Downloading Amazon Reviews dataset...\")\n",
        "    path = kagglehub.dataset_download(\"kritanjalijain/amazon-reviews\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # Find and extract zip files if needed\n",
        "    zip_files = [f for f in os.listdir(path) if f.endswith('.zip')]\n",
        "\n",
        "    if zip_files:\n",
        "        for zip_file in zip_files:\n",
        "            zip_path = os.path.join(path, zip_file)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                print(f\"Extracting {zip_file}...\")\n",
        "                z.extractall(path)\n",
        "\n",
        "    return\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "CYpk5P9kNEIW",
        "outputId": "0d8d0716-a546-41c3-dbd2-a548a7c83205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def load_data(\n",
        "    dataset_path: str,\n",
        "    n_users: int = 1000,\n",
        "    fake_seed: int = 42,\n",
        "    text_feat_max: int = 100,\n",
        "    text_svd_components: int = 20,\n",
        "    max_reviews: int = 100000\n",
        ") -> Tuple[pd.DataFrame, dict, dict, Optional[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Load Amazon reviews (JSON or CSV), fake user IDs up front to avoid warnings,\n",
        "    add extra features, and extract optional TF-IDF + SVD text features.\n",
        "\n",
        "    Returns:\n",
        "      df: DataFrame with columns ['user_id','item_id','rating','text_length','title_length',...]\n",
        "      user_id_map: original user_id → index\n",
        "      item_id_map: original item_id → index\n",
        "      text_features: array of shape (n_samples, text_svd_components) or None\n",
        "    \"\"\"\n",
        "    # 1) Load raw data\n",
        "    json_files = [f for f in os.listdir(dataset_path) if f.endswith('.json')]\n",
        "    csv_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
        "\n",
        "    if json_files:\n",
        "        file_path = os.path.join(dataset_path, json_files[0])\n",
        "        data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= max_reviews:\n",
        "                    break\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "        df = pd.DataFrame(data)\n",
        "    elif csv_files:\n",
        "        file_path = os.path.join(dataset_path, csv_files[0])\n",
        "        df = pd.read_csv(file_path, nrows=max_reviews)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No JSON or CSV files found in the dataset path\")\n",
        "\n",
        "    # 2) Immediately generate fake user_ids to ensure 'user_id' column exists\n",
        "    Faker.seed(fake_seed)\n",
        "    fake = Faker()\n",
        "    unique_fake_ids = [fake.uuid4() for _ in range(n_users)]\n",
        "    df['user_id'] = [random.choice(unique_fake_ids) for _ in range(len(df))]\n",
        "    logging.info(f\"Pre-assigned {n_users} fake users across {len(df)} records; 'user_id' column now present.\")\n",
        "\n",
        "    # 3) Identify core columns (now 'user_id' is guaranteed)\n",
        "    user_col = 'user_id'\n",
        "    # item detection\n",
        "    if 'asin' in df.columns:\n",
        "        item_col = 'asin'\n",
        "    elif 'product_id' in df.columns:\n",
        "        item_col = 'product_id'\n",
        "    else:\n",
        "        item_col = df.columns[1]\n",
        "    # rating detection\n",
        "    if 'overall' in df.columns:\n",
        "        rating_col = 'overall'\n",
        "    elif 'rating' in df.columns:\n",
        "        rating_col = 'rating'\n",
        "    else:\n",
        "        rating_col = None\n",
        "        for col in df.columns:\n",
        "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].min() >= 0 and df[col].max() <= 5:\n",
        "                rating_col = col\n",
        "                break\n",
        "    # text & title\n",
        "    text_col = 'reviewText' if 'reviewText' in df.columns else 'text' if 'text' in df.columns else None\n",
        "    title_col = 'summary' if 'summary' in df.columns else 'title' if 'title' in df.columns else None\n",
        "\n",
        "    # 4) Keep and rename necessary columns\n",
        "    cols = [c for c in [user_col, item_col, rating_col, text_col, title_col] if c]\n",
        "    df = df[cols].copy()\n",
        "    mapping = {user_col: 'user_id', item_col: 'item_id'}\n",
        "    if rating_col:\n",
        "        mapping[rating_col] = 'rating'\n",
        "    df.rename(columns=mapping, inplace=True)\n",
        "\n",
        "    # 5) Drop missing critical values\n",
        "    df.dropna(subset=['user_id', 'item_id'], inplace=True)\n",
        "    if 'rating' in df.columns:\n",
        "        df.dropna(subset=['rating'], inplace=True)\n",
        "        df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "    # 6) Add extra numeric features\n",
        "    if text_col:\n",
        "        df['text_length'] = df[text_col].fillna('').astype(str).apply(len)\n",
        "    if title_col:\n",
        "        df['title_length'] = df[title_col].fillna('').astype(str).apply(len)\n",
        "\n",
        "    # 7) Create index maps\n",
        "    user_ids = df['user_id'].unique()\n",
        "    item_ids = df['item_id'].unique()\n",
        "    user_id_map = {u: i for i, u in enumerate(user_ids)}\n",
        "    item_id_map = {v: j for j, v in enumerate(item_ids)}\n",
        "    df['user_idx'] = df['user_id'].map(user_id_map)\n",
        "    df['item_idx'] = df['item_id'].map(item_id_map)\n",
        "\n",
        "    # 8) Extract text features if possible\n",
        "    text_features = None\n",
        "    if text_col:\n",
        "        vec = TfidfVectorizer(max_features=text_feat_max, stop_words='english')\n",
        "        sample = df[text_col].fillna('').astype(str)\n",
        "        tfidf = vec.fit_transform(sample)\n",
        "        svd = TruncatedSVD(n_components=text_svd_components, random_state=fake_seed)\n",
        "        text_features = svd.fit_transform(tfidf)\n",
        "        logging.info(f\"Extracted text_features shape={text_features.shape}\")\n",
        "\n",
        "    return df, user_id_map, item_id_map, text_features\n"
      ],
      "metadata": {
        "id": "N99Q3VeML1cT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cold start scenario\n",
        "def create_cold_start_scenario(df, cold_start_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create a cold start scenario by hiding interactions for some users\n",
        "    \"\"\"\n",
        "    # Identify users with few interactions (naturally cold)\n",
        "    user_counts = df['user_idx'].value_counts()\n",
        "\n",
        "    # Find users with at least 5 interactions\n",
        "    qualified_users = user_counts[user_counts >= 5].index.tolist()\n",
        "\n",
        "    if len(qualified_users) == 0:\n",
        "        print(\"Warning: No users with enough interactions for cold start simulation\")\n",
        "        # Fall back to random selection\n",
        "        all_users = df['user_idx'].unique()\n",
        "        cold_users = np.random.choice(\n",
        "            all_users,\n",
        "            size=int(len(all_users) * cold_start_ratio),\n",
        "            replace=False\n",
        "        )\n",
        "    else:\n",
        "        # Select a subset of users to simulate as cold start\n",
        "        cold_users = np.random.choice(\n",
        "            qualified_users,\n",
        "            size=int(len(qualified_users) * cold_start_ratio),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "    print(f\"Selected {len(cold_users)} users as cold start users\")\n",
        "\n",
        "    # For cold start users, keep only 1-2 interactions as \"observed\" and move the rest to \"test\"\n",
        "    train_df = pd.DataFrame()\n",
        "    test_df = pd.DataFrame()\n",
        "\n",
        "    for user_idx in df['user_idx'].unique():\n",
        "        user_data = df[df['user_idx'] == user_idx].copy()\n",
        "\n",
        "        if user_idx in cold_users:\n",
        "            # For cold users, keep only 1-2 interactions as observed\n",
        "            n_observed = min(2, len(user_data))\n",
        "            observed = user_data.sample(n_observed)\n",
        "            holdout = user_data.drop(observed.index)\n",
        "\n",
        "            train_df = pd.concat([train_df, observed])\n",
        "            test_df = pd.concat([test_df, holdout])\n",
        "        else:\n",
        "            # For warm users, use 80/20 split\n",
        "            user_train, user_test = train_test_split(user_data, test_size=0.2, random_state=42)\n",
        "            train_df = pd.concat([train_df, user_train])\n",
        "            test_df = pd.concat([test_df, user_test])\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} interactions\")\n",
        "    print(f\"Test set: {len(test_df)} interactions\")\n",
        "\n",
        "    # Create a mask to identify cold users\n",
        "    cold_user_mask = np.isin(df['user_idx'].unique(), cold_users)\n",
        "\n",
        "    return train_df, test_df, cold_user_mask, cold_users\n",
        "\n"
      ],
      "metadata": {
        "id": "xRA1JNGoSZQM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Factorization model\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        # User and item embedding matrices\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.user_factors.weight.data.normal_(0, 0.1)\n",
        "        self.item_factors.weight.data.normal_(0, 0.1)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Lookup embeddings\n",
        "        user_embedding = self.user_factors(user_indices)\n",
        "        item_embedding = self.item_factors(item_indices)\n",
        "\n",
        "        # Element-wise product and sum\n",
        "        prediction = (user_embedding * item_embedding).sum(dim=1)\n",
        "        return prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "hlkdSe6BSf3J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-Paced Matrix Factorization\n",
        "class SelfPacedMatrixFactorization:\n",
        "    def __init__(self, n_users, n_items, n_factors=20,\n",
        "                 lambda_start=0.1, lambda_end=10.0, lambda_steps=5,\n",
        "                 learning_rate=0.01, weight_decay=0.001, batch_size=256,\n",
        "                 device='cpu'):\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "        self.lambda_start = lambda_start\n",
        "        self.lambda_end = lambda_end\n",
        "        self.lambda_steps = lambda_steps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # Create MF model\n",
        "        self.model = MatrixFactorization(n_users, n_items, n_factors).to(device)\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        self.loss_fn = nn.MSELoss(reduction='none')  # Need sample-wise loss for self-paced learning\n",
        "\n",
        "        # Lambda schedule (pace parameter)\n",
        "        self.lambda_schedule = np.logspace(\n",
        "            np.log10(lambda_start),\n",
        "            np.log10(lambda_end),\n",
        "            lambda_steps\n",
        "        )\n",
        "\n",
        "        # Track loss history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def fit(self, train_df, val_df=None, epochs=10, verbose=True):\n",
        "        \"\"\"\n",
        "        Train the model using self-paced learning\n",
        "        \"\"\"\n",
        "        # Convert dataframes to tensors\n",
        "        train_users = torch.LongTensor(train_df['user_idx'].values).to(self.device)\n",
        "        train_items = torch.LongTensor(train_df['item_idx'].values).to(self.device)\n",
        "        train_ratings = torch.FloatTensor(train_df['rating'].values).to(self.device)\n",
        "\n",
        "        if val_df is not None:\n",
        "            val_users = torch.LongTensor(val_df['user_idx'].values).to(self.device)\n",
        "            val_items = torch.LongTensor(val_df['item_idx'].values).to(self.device)\n",
        "            val_ratings = torch.FloatTensor(val_df['rating'].values).to(self.device)\n",
        "\n",
        "        n_samples = len(train_df)\n",
        "        n_batches = (n_samples + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        # Initial weights (all samples equally weighted)\n",
        "        sample_weights = torch.ones(n_samples, device=self.device)\n",
        "\n",
        "        # Training loop\n",
        "        for lambda_pace in self.lambda_schedule:\n",
        "            if verbose:\n",
        "                print(f\"\\nTraining with pace parameter λ={lambda_pace:.4f}\")\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                # Shuffle data\n",
        "                indices = torch.randperm(n_samples)\n",
        "                train_users_shuffled = train_users[indices]\n",
        "                train_items_shuffled = train_items[indices]\n",
        "                train_ratings_shuffled = train_ratings[indices]\n",
        "                sample_weights_shuffled = sample_weights[indices]\n",
        "\n",
        "                # Training\n",
        "                self.model.train()\n",
        "                epoch_loss = 0.0\n",
        "\n",
        "                for batch in range(n_batches):\n",
        "                    start_idx = batch * self.batch_size\n",
        "                    end_idx = min((batch + 1) * self.batch_size, n_samples)\n",
        "\n",
        "                    if end_idx <= start_idx:\n",
        "                        continue\n",
        "\n",
        "                    batch_users = train_users_shuffled[start_idx:end_idx]\n",
        "                    batch_items = train_items_shuffled[start_idx:end_idx]\n",
        "                    batch_ratings = train_ratings_shuffled[start_idx:end_idx]\n",
        "                    batch_weights = sample_weights_shuffled[start_idx:end_idx]\n",
        "\n",
        "                    # Forward pass\n",
        "                    self.optimizer.zero_grad()\n",
        "                    predictions = self.model(batch_users, batch_items)\n",
        "\n",
        "                    # Calculate loss with sample weights\n",
        "                    sample_loss = self.loss_fn(predictions, batch_ratings)\n",
        "                    weighted_loss = (sample_loss * batch_weights).mean()\n",
        "\n",
        "                    # Backward and optimize\n",
        "                    weighted_loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    epoch_loss += weighted_loss.item() * (end_idx - start_idx)\n",
        "\n",
        "                epoch_loss /= n_samples\n",
        "                self.train_losses.append(epoch_loss)\n",
        "\n",
        "                # Validation\n",
        "                val_loss = 0.0\n",
        "                if val_df is not None:\n",
        "                    val_loss = self.evaluate(val_df)\n",
        "                    self.val_losses.append(val_loss)\n",
        "\n",
        "                if verbose and (epoch % 5 == 0 or epoch == epochs - 1):\n",
        "                    val_msg = f\", Val Loss: {val_loss:.4f}\" if val_df is not None else \"\"\n",
        "                    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}{val_msg}\")\n",
        "\n",
        "            # Update sample weights using self-paced learning\n",
        "            with torch.no_grad():\n",
        "                all_predictions = self.model(train_users, train_items)\n",
        "                all_losses = self.loss_fn(all_predictions, train_ratings)\n",
        "\n",
        "                # Self-paced learning weight update\n",
        "                # v_i = 1 if l_i <= λ, else 0 (hard self-paced learning)\n",
        "                # For smoother training, we can use a soft version:\n",
        "                # v_i = max(0, 1 - l_i/λ)\n",
        "\n",
        "                # Hard version\n",
        "                # sample_weights = (all_losses <= lambda_pace).float()\n",
        "\n",
        "                # Soft version\n",
        "                sample_weights = torch.clamp(1 - all_losses / lambda_pace, min=0.0)\n",
        "\n",
        "                # Log the number of \"easy\" samples\n",
        "                n_easy = (sample_weights > 0.5).sum().item()\n",
        "                if verbose:\n",
        "                    print(f\"λ={lambda_pace:.4f}: {n_easy}/{n_samples} samples considered 'easy' ({n_easy/n_samples:.1%})\")\n",
        "\n",
        "    def evaluate(self, val_df):\n",
        "        \"\"\"\n",
        "        Evaluate the model on validation data\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        val_users = torch.LongTensor(val_df['user_idx'].values).to(self.device)\n",
        "        val_items = torch.LongTensor(val_df['item_idx'].values).to(self.device)\n",
        "        val_ratings = torch.FloatTensor(val_df['rating'].values).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(val_users, val_items)\n",
        "            loss = nn.MSELoss()(predictions, val_ratings)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def predict(self, user_indices, item_indices):\n",
        "        \"\"\"\n",
        "        Make predictions for user-item pairs\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        user_tensor = torch.LongTensor(user_indices).to(self.device)\n",
        "        item_tensor = torch.LongTensor(item_indices).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(user_tensor, item_tensor)\n",
        "\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "    def plot_loss_history(self):\n",
        "        \"\"\"\n",
        "        Plot training and validation loss history\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "\n",
        "        if self.val_losses:\n",
        "            plt.plot(self.val_losses, label='Validation Loss')\n",
        "\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hhi3YwSTUq4e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semi-supervised learning with pseudo-labeling\n",
        "class SemiSupervisedRecommender:\n",
        "    def __init__(self, base_model, n_users, n_items, confidence_threshold=0.8):\n",
        "        self.base_model = base_model\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "\n",
        "        # For clustering users\n",
        "        self.user_clusters = None\n",
        "        self.item_clusters = None\n",
        "\n",
        "    def cluster_users(self, train_df, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Cluster users based on their item interactions\n",
        "        \"\"\"\n",
        "        # Create user-item interaction matrix\n",
        "        user_item_matrix = np.zeros((self.n_users, self.n_items))\n",
        "\n",
        "        for _, row in train_df.iterrows():\n",
        "            user_idx = row['user_idx']\n",
        "            item_idx = row['item_idx']\n",
        "            rating = row['rating']\n",
        "            user_item_matrix[user_idx, item_idx] = rating\n",
        "\n",
        "        # Normalize rows (users)\n",
        "        row_sums = user_item_matrix.sum(axis=1)\n",
        "        mask = row_sums > 0\n",
        "        user_item_matrix[mask] = user_item_matrix[mask] / row_sums[mask, np.newaxis]\n",
        "\n",
        "        # Cluster users\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        self.user_clusters = kmeans.fit_predict(user_item_matrix)\n",
        "\n",
        "        # Also cluster items by their user interactions (transpose)\n",
        "        item_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        self.item_clusters = item_kmeans.fit_predict(user_item_matrix.T)\n",
        "\n",
        "        return self.user_clusters, self.item_clusters\n",
        "\n",
        "    def generate_pseudo_labels(self, train_df, cold_users, top_k=10, bottom_k=5):\n",
        "        \"\"\"\n",
        "        Generate pseudo labels for cold start users based on similar users\n",
        "        \"\"\"\n",
        "        if self.user_clusters is None:\n",
        "            self.cluster_users(train_df)\n",
        "\n",
        "        # Create user-item matrix for existing ratings\n",
        "        user_item_matrix = np.zeros((self.n_users, self.n_items))\n",
        "        for _, row in train_df.iterrows():\n",
        "            user_idx = row['user_idx']\n",
        "            item_idx = row['item_idx']\n",
        "            rating = row['rating']\n",
        "            user_item_matrix[user_idx, item_idx] = rating\n",
        "\n",
        "        # Get average item ratings per cluster\n",
        "        cluster_item_avg = np.zeros((np.max(self.user_clusters) + 1, self.n_items))\n",
        "        cluster_item_count = np.zeros((np.max(self.user_clusters) + 1, self.n_items))\n",
        "\n",
        "        for user_idx in range(self.n_users):\n",
        "            if user_idx in cold_users:\n",
        "                continue\n",
        "\n",
        "            cluster_id = self.user_clusters[user_idx]\n",
        "            for item_idx in range(self.n_items):\n",
        "                rating = user_item_matrix[user_idx, item_idx]\n",
        "                if rating > 0:\n",
        "                    cluster_item_avg[cluster_id, item_idx] += rating\n",
        "                    cluster_item_count[cluster_id, item_idx] += 1\n",
        "\n",
        "        # Calculate average (avoid division by zero)\n",
        "        cluster_item_count[cluster_item_count == 0] = 1\n",
        "        cluster_item_avg = cluster_item_avg / cluster_item_count\n",
        "\n",
        "        # Generate pseudo labels for cold users\n",
        "        pseudo_labeled_data = []\n",
        "\n",
        "        for user_idx in cold_users:\n",
        "            # Get user's cluster based on their limited interactions\n",
        "            user_cluster = self.user_clusters[user_idx]\n",
        "\n",
        "            # Get cluster's average ratings\n",
        "            cluster_avg_ratings = cluster_item_avg[user_cluster]\n",
        "\n",
        "            # Find items with highest and lowest ratings in the cluster\n",
        "            # (excluding items the user has already rated)\n",
        "            rated_items = set(train_df[train_df['user_idx'] == user_idx]['item_idx'])\n",
        "            unrated_items = np.array([i for i in range(self.n_items) if i not in rated_items])\n",
        "\n",
        "            if len(unrated_items) == 0:\n",
        "                continue\n",
        "\n",
        "            # Get cluster ratings for unrated items\n",
        "            unrated_ratings = cluster_avg_ratings[unrated_items]\n",
        "\n",
        "            # Select top-k and bottom-k items\n",
        "            top_indices = unrated_items[np.argsort(-unrated_ratings)[:top_k]]\n",
        "            bottom_indices = unrated_items[np.argsort(unrated_ratings)[:bottom_k]]\n",
        "\n",
        "            # Create pseudo labels (high rating for top items, low for bottom)\n",
        "            for item_idx in top_indices:\n",
        "                if cluster_avg_ratings[item_idx] > 0:\n",
        "                    pseudo_labeled_data.append({\n",
        "                        'user_idx': user_idx,\n",
        "                        'item_idx': item_idx,\n",
        "                        'rating': min(5.0, cluster_avg_ratings[item_idx] * 1.1),  # Slightly higher than cluster avg\n",
        "                        'is_pseudo': 1\n",
        "                    })\n",
        "\n",
        "            for item_idx in bottom_indices:\n",
        "                if cluster_avg_ratings[item_idx] > 0:\n",
        "                    pseudo_labeled_data.append({\n",
        "                        'user_idx': user_idx,\n",
        "                        'item_idx': item_idx,\n",
        "                        'rating': max(1.0, cluster_avg_ratings[item_idx] * 0.9),  # Slightly lower than cluster avg\n",
        "                        'is_pseudo': 1\n",
        "                    })\n",
        "\n",
        "        pseudo_df = pd.DataFrame(pseudo_labeled_data)\n",
        "        print(f\"Generated {len(pseudo_df)} pseudo labels for {len(cold_users)} cold start users\")\n",
        "\n",
        "        return pseudo_df\n",
        "\n",
        "    def train_semi_supervised(self, train_df, test_df, cold_users,\n",
        "                              epochs_per_stage=5, total_stages=3):\n",
        "        \"\"\"\n",
        "        Train model using semi-supervised approach with self-paced learning\n",
        "        \"\"\"\n",
        "        # First train the base model on labeled data\n",
        "        print(\"\\nStage 1: Training on labeled data only\")\n",
        "        self.base_model.fit(train_df, val_df=None, epochs=epochs_per_stage)\n",
        "\n",
        "        # Initialize training with pseudo-labels\n",
        "        augmented_train_df = train_df.copy()\n",
        "\n",
        "        for stage in range(2, total_stages + 1):\n",
        "            print(f\"\\nStage {stage}: Generating pseudo-labels and retraining\")\n",
        "\n",
        "            # Generate pseudo-labels\n",
        "            pseudo_df = self.generate_pseudo_labels(\n",
        "                augmented_train_df,\n",
        "                cold_users,\n",
        "                top_k=5 * stage,  # Increase number of pseudo-labels with each stage\n",
        "                bottom_k=3 * stage\n",
        "            )\n",
        "\n",
        "            if len(pseudo_df) == 0:\n",
        "                print(\"No pseudo-labels generated, skipping stage\")\n",
        "                continue\n",
        "\n",
        "            # Add pseudo-labels to training data\n",
        "            augmented_train_df = pd.concat([train_df, pseudo_df])\n",
        "\n",
        "            # Make sure pseudo-labels have a weight column (for self-paced learning)\n",
        "            if 'is_pseudo' not in augmented_train_df:\n",
        "                augmented_train_df['is_pseudo'] = 0\n",
        "\n",
        "            # Train with augmented data\n",
        "            self.base_model.fit(augmented_train_df, val_df=None, epochs=epochs_per_stage)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            rmse, mae = self.evaluate(test_df)\n",
        "            print(f\"Stage {stage} Test RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
        "\n",
        "            # Evaluate specifically for cold users\n",
        "            cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "            if len(cold_test_df) > 0:\n",
        "                cold_rmse, cold_mae = self.evaluate(cold_test_df)\n",
        "                print(f\"Stage {stage} Cold Users Test RMSE: {cold_rmse:.4f}, MAE: {cold_mae:.4f}\")\n",
        "\n",
        "    def evaluate(self, test_df):\n",
        "        \"\"\"\n",
        "        Evaluate model on test data\n",
        "        \"\"\"\n",
        "        user_indices = test_df['user_idx'].values\n",
        "        item_indices = test_df['item_idx'].values\n",
        "        true_ratings = test_df['rating'].values\n",
        "\n",
        "        # Get predictions\n",
        "        pred_ratings = self.base_model.predict(user_indices, item_indices)\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings))\n",
        "        mae = mean_absolute_error(true_ratings, pred_ratings)\n",
        "\n",
        "        return rmse, mae\n",
        "\n",
        "    def get_recommendations(self, user_idx, n_recommendations=10, exclude_rated=True):\n",
        "        \"\"\"\n",
        "        Get top N recommendations for a user\n",
        "        \"\"\"\n",
        "        # Get all items\n",
        "        all_items = np.arange(self.n_items)\n",
        "\n",
        "        # If excluding rated items, filter them out\n",
        "        if exclude_rated:\n",
        "            # This requires the original training dataframe to be available\n",
        "            # You might want to store this information during initialization\n",
        "            pass\n",
        "\n",
        "        # Predict ratings for all items\n",
        "        user_indices = np.full_like(all_items, user_idx)\n",
        "        predicted_ratings = self.base_model.predict(user_indices, all_items)\n",
        "\n",
        "        # Get top N items by predicted rating\n",
        "        top_item_indices = np.argsort(-predicted_ratings)[:n_recommendations]\n",
        "        top_ratings = predicted_ratings[top_item_indices]\n",
        "\n",
        "        return list(zip(top_item_indices, top_ratings))\n",
        "\n"
      ],
      "metadata": {
        "id": "zOuF9trdUxpg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation utilities\n",
        "def calculate_precision_recall_at_k(predictions, test_df, k=10):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K and Recall@K\n",
        "    \"\"\"\n",
        "    user_true_items = {}\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_idx = row['user_idx']\n",
        "        item_idx = row['item_idx']\n",
        "        rating = row['rating']\n",
        "\n",
        "        if rating >= 4.0:  # Consider as relevant if rating >= 4\n",
        "            if user_idx not in user_true_items:\n",
        "                user_true_items[user_idx] = set()\n",
        "            user_true_items[user_idx].add(item_idx)\n",
        "\n",
        "    precision_sum = 0\n",
        "    recall_sum = 0\n",
        "    user_count = 0\n",
        "\n",
        "    for user_idx, user_predictions in predictions.items():\n",
        "        if user_idx not in user_true_items:\n",
        "            continue\n",
        "\n",
        "        # Get top k predictions\n",
        "        recommended_items = [item for item, _ in user_predictions[:k]]\n",
        "        relevant_items = user_true_items[user_idx]\n",
        "\n",
        "        if not relevant_items:\n",
        "            continue\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        n_relevant_recommended = len(set(recommended_items) & relevant_items)\n",
        "        precision = n_relevant_recommended / min(k, len(recommended_items)) if recommended_items else 0\n",
        "        recall = n_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
        "\n",
        "        precision_sum += precision\n",
        "        recall_sum += recall\n",
        "        user_count += 1\n",
        "\n",
        "    # Calculate average\n",
        "    avg_precision = precision_sum / user_count if user_count > 0 else 0\n",
        "    avg_recall = recall_sum / user_count if user_count > 0 else 0\n",
        "\n",
        "    return avg_precision, avg_recall\n",
        "\n"
      ],
      "metadata": {
        "id": "wPmwO6fcU43O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ndcg_at_k(predictions, test_df, k=10):\n",
        "    \"\"\"\n",
        "    Calculate NDCG@K\n",
        "    \"\"\"\n",
        "    # Create user-item-rating dictionary from test data\n",
        "    user_item_ratings = {}\n",
        "    for _, row in test_df.iterrows():\n",
        "        user_idx = row['user_idx']\n",
        "        item_idx = row['item_idx']\n",
        "        rating = row['rating']\n",
        "\n",
        "        if user_idx not in user_item_ratings:\n",
        "            user_item_ratings[user_idx] = {}\n",
        "        user_item_ratings[user_idx][item_idx] = rating\n",
        "\n",
        "    ndcg_sum = 0\n",
        "    user_count = 0\n",
        "\n",
        "    for user_idx, user_predictions in predictions.items():\n",
        "        if user_idx not in user_item_ratings:\n",
        "            continue\n",
        "\n",
        "        # Get predicted items and their scores\n",
        "        pred_items = []\n",
        "        pred_scores = []\n",
        "        for item_idx, score in user_predictions[:k]:\n",
        "            pred_items.append(item_idx)\n",
        "            pred_scores.append(score)\n",
        "\n",
        "        # Get true ratings for predicted items (0 if not rated)\n",
        "        true_scores = []\n",
        "        for item_idx in pred_items:\n",
        "            true_scores.append(user_item_ratings[user_idx].get(item_idx, 0))\n",
        "\n",
        "        # Calculate NDCG\n",
        "        if sum(true_scores) > 0:  # Skip if no relevant items\n",
        "            ndcg = ndcg_score([true_scores], [pred_scores])\n",
        "            ndcg_sum += ndcg\n",
        "            user_count += 1\n",
        "\n",
        "    # Calculate average NDCG\n",
        "    avg_ndcg = ndcg_sum / user_count if user_count > 0 else 0\n",
        "\n",
        "    return avg_ndcg\n",
        "\n"
      ],
      "metadata": {
        "id": "cIaGLQmPU9pj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_time_to_quality(model, train_df, test_df, cold_users,\n",
        "                            epochs=[1, 5, 10, 20, 50], n_recommendations=10):\n",
        "    \"\"\"\n",
        "    Evaluate how quickly the model improves for cold start users\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Initial evaluation without training\n",
        "    cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "    rmse_0, mae_0 = 0, 0  # No predictions yet\n",
        "    results.append({\n",
        "        'epochs': 0,\n",
        "        'rmse': rmse_0,\n",
        "        'mae': mae_0\n",
        "    })\n",
        "\n",
        "    # Train and evaluate incrementally\n",
        "    cumulative_epochs = 0\n",
        "\n",
        "    for n_epochs in epochs:\n",
        "        # Train for additional epochs\n",
        "        model.base_model.fit(train_df, epochs=n_epochs, verbose=False)\n",
        "        cumulative_epochs += n_epochs\n",
        "\n",
        "        # Evaluate\n",
        "        rmse, mae = model.evaluate(cold_test_df)\n",
        "\n",
        "        results.append({\n",
        "            'epochs': cumulative_epochs,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "b6co6TkBVBer"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker  # For fake user_id generation\n",
        "import logging\n",
        "import random\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def parse_args(argv = None) -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Sentiment classification with fake user IDs and text features\"\n",
        "    )\n",
        "    parser.add_argument(\"--input\", required=True, help=\"Path to input CSV file with columns: polarity,title,text\")\n",
        "    parser.add_argument(\"--n_users\", type=int, default=1000, help=\"Number of unique fake users to simulate\")\n",
        "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"Test set fraction\")\n",
        "    parser.add_argument(\"--random_seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size for PyTorch DataLoader\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n",
        "    return parser.parse_args(argv)\n",
        "\n",
        "def setup_logging():\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    )\n",
        "\n",
        "# Main function to run the complete pipeline\n",
        "def main():\n",
        "    print(\"Starting User Cold Start Recommendation System\")\n",
        "\n",
        "    # 1. Download and load the dataset\n",
        "    dataset_path = download_and_extract_dataset()\n",
        "    # Create an argument parser\n",
        "    parser = argparse.ArgumentParser(description=\"User Cold Start Recommendation System\")\n",
        "    # Define the input argument\n",
        "    parser.add_argument(\"--input\", required=True, help=\"Path to the input dataset\")\n",
        "    parser.add_argument(\"--n_users\", type=int, default=500, help=\"Number of unique fake users to simulate\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--random_seed\", type=int, default=3, help=\"seeder for randomization\")\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args([\n",
        "        \"--input\", dataset_path,\n",
        "        \"--n_users\", \"500\",\n",
        "        \"--epochs\", \"3\",\n",
        "        \"--random_seed\", \"42\"\n",
        "    ])\n",
        "    setup_logging()\n",
        "    np.random.seed(args.random_seed)\n",
        "    random.seed(args.random_seed)\n",
        "    torch.manual_seed(args.random_seed)\n",
        "\n",
        "    # df = load_and_prepare(args.input, args.n_users, args.random_seed)\n",
        "\n",
        "    df, user_id_map, item_id_map, text_features = load_data(dataset_path)\n",
        "\n",
        "    # 2. Create cold start scenario\n",
        "    train_df, test_df, cold_user_mask, cold_users = create_cold_start_scenario(df)\n",
        "\n",
        "    # Print some statistics\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Total users: {len(user_id_map)}\")\n",
        "    print(f\"Total items: {len(item_id_map)}\")\n",
        "    print(f\"Total interactions: {len(df)}\")\n",
        "    print(f\"Cold start users: {sum(cold_user_mask)} ({sum(cold_user_mask)/len(user_id_map):.1%})\")\n",
        "\n",
        "    # 3. Initialize models\n",
        "    n_users = len(user_id_map)\n",
        "    n_items = len(item_id_map)\n",
        "\n",
        "    print(\"\\nInitializing Self-Paced Matrix Factorization model...\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    spmf = SelfPacedMatrixFactorization(\n",
        "        n_users=n_users,\n",
        "        n_items=n_items,\n",
        "        n_factors=30,\n",
        "        lambda_start=0.1,\n",
        "        lambda_end=5.0,\n",
        "        lambda_steps=3,\n",
        "        learning_rate=0.01,\n",
        "        weight_decay=0.001,\n",
        "        batch_size=256,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # 4. Initialize semi-supervised recommender\n",
        "    print(\"\\nInitializing Semi-supervised Recommender...\")\n",
        "    semi_supervised = SemiSupervisedRecommender(\n",
        "        base_model=spmf,\n",
        "        n_users=n_users,\n",
        "        n_items=n_items,\n",
        "        confidence_threshold=0.8\n",
        "    )\n",
        "\n",
        "    # 5. Train semi-supervised model\n",
        "    print(\"\\nTraining semi-supervised model...\")\n",
        "    semi_supervised.train_semi_supervised(\n",
        "        train_df=train_df,\n",
        "        test_df=test_df,\n",
        "        cold_users=cold_users,\n",
        "        epochs_per_stage=5,\n",
        "        total_stages=3\n",
        "    )\n",
        "\n",
        "    # 6. Evaluate model performance\n",
        "    print(\"\\nEvaluating model performance...\")\n",
        "\n",
        "    # Overall performance\n",
        "    overall_rmse, overall_mae = semi_supervised.evaluate(test_df)\n",
        "    print(f\"Overall Test RMSE: {overall_rmse:.4f}, MAE: {overall_mae:.4f}\")\n",
        "\n",
        "    # Performance on cold users\n",
        "    cold_test_df = test_df[test_df['user_idx'].isin(cold_users)]\n",
        "    if len(cold_test_df) > 0:\n",
        "        cold_rmse, cold_mae = semi_supervised.evaluate(cold_test_df)\n",
        "        print(f\"Cold Users Test RMSE: {cold_rmse:.4f}, MAE: {cold_mae:.4f}\")\n",
        "\n",
        "    # 7. Generate recommendations\n",
        "    print(\"\\nGenerating recommendations for sample users...\")\n",
        "\n",
        "    # Sample a few users for recommendation examples\n",
        "    sample_users = np.random.choice(cold_users, min(5, len(cold_users)), replace=False)\n",
        "\n",
        "    user_recommendations = {}\n",
        "    for user_idx in sample_users:\n",
        "        recommendations = semi_supervised.get_recommendations(user_idx, n_recommendations=10)\n",
        "        user_recommendations[user_idx] = recommendations\n",
        "\n",
        "        # Map back to original user/item IDs for display\n",
        "        rev_user_id_map = {v: k for k, v in user_id_map.items()}\n",
        "        rev_item_id_map = {v: k for k, v in item_id_map.items()}\n",
        "\n",
        "        user_id = rev_user_id_map.get(user_idx, f\"User_{user_idx}\")\n",
        "        print(f\"\\nTop 10 recommendations for user {user_id}:\")\n",
        "\n",
        "        for i, (item_idx, score) in enumerate(recommendations, 1):\n",
        "            item_id = rev_item_id_map.get(item_idx, f\"Item_{item_idx}\")\n",
        "            print(f\"{i}. Item {item_id} - Score: {score:.2f}\")\n",
        "\n",
        "    # 8. Analyze time-to-quality\n",
        "    print(\"\\nAnalyzing time-to-quality for cold users...\")\n",
        "\n",
        "    # Create a fresh model for this analysis\n",
        "    fresh_spmf = SelfPacedMatrixFactorization(\n",
        "        n_users=n_users,\n",
        "        n_items=n_items,\n",
        "        n_factors=30,\n",
        "        lambda_start=0.1,\n",
        "        lambda_end=5.0,\n",
        "        lambda_steps=3,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    fresh_semi = SemiSupervisedRecommender(\n",
        "        base_model=fresh_spmf,\n",
        "        n_users=n_users,\n",
        "        n_items=n_items\n",
        "    )\n",
        "\n",
        "    ttq_results = evaluate_time_to_quality(\n",
        "        model=fresh_semi,\n",
        "        train_df=train_df,\n",
        "        test_df=test_df,\n",
        "        cold_users=cold_users,\n",
        "        epochs=[1, 4, 5, 10]  # Smaller for demonstration\n",
        "    )\n",
        "\n",
        "    print(\"\\nTime-to-quality results:\")\n",
        "    print(ttq_results)\n",
        "\n",
        "    # 9. Plot loss history\n",
        "    print(\"\\nPlotting loss history...\")\n",
        "    spmf.plot_loss_history()\n",
        "\n",
        "    print(\"\\nRecommendation system training and evaluation complete!\")\n",
        "\n",
        "    return {\n",
        "        \"model\": semi_supervised,\n",
        "        \"metrics\": {\n",
        "            \"overall_rmse\": overall_rmse,\n",
        "            \"overall_mae\": overall_mae,\n",
        "            \"cold_rmse\": cold_rmse if len(cold_test_df) > 0 else None,\n",
        "            \"cold_mae\": cold_mae if len(cold_test_df) > 0 else None\n",
        "        },\n",
        "        \"time_to_quality\": ttq_results\n",
        "    }\n",
        "\n",
        "# Entry point for the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXk7V-_rVFnU",
        "outputId": "cebcf444-4768-42de-a7cb-9dc067d6c640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting User Cold Start Recommendation System\n",
            "Downloading Amazon Reviews dataset...\n",
            "Dataset downloaded to: /kaggle/input/amazon-reviews\n",
            "Selected 200 users as cold start users\n",
            "Train set: 64157 interactions\n",
            "Test set: 35838 interactions\n",
            "\n",
            "Dataset Statistics:\n",
            "Total users: 1000\n",
            "Total items: 86457\n",
            "Total interactions: 99995\n",
            "Cold start users: 200 (20.0%)\n",
            "\n",
            "Initializing Self-Paced Matrix Factorization model...\n",
            "Using device: cpu\n",
            "\n",
            "Initializing Semi-supervised Recommender...\n",
            "\n",
            "Training semi-supervised model...\n",
            "\n",
            "Stage 1: Training on labeled data only\n",
            "\n",
            "Training with pace parameter λ=0.1000\n",
            "Epoch 1/5, Train Loss: 2.5419\n",
            "Epoch 5/5, Train Loss: 2.5418\n",
            "λ=0.1000: 0/64157 samples considered 'easy' (0.0%)\n",
            "\n",
            "Training with pace parameter λ=0.7071\n",
            "Epoch 1/5, Train Loss: 0.0000\n",
            "Epoch 5/5, Train Loss: 0.0000\n",
            "λ=0.7071: 0/64157 samples considered 'easy' (0.0%)\n",
            "\n",
            "Training with pace parameter λ=5.0000\n",
            "Epoch 1/5, Train Loss: 0.0000\n",
            "Epoch 5/5, Train Loss: 0.0000\n",
            "λ=5.0000: 31184/64157 samples considered 'easy' (48.6%)\n",
            "\n",
            "Stage 2: Generating pseudo-labels and retraining\n",
            "Generated 1910 pseudo labels for 200 cold start users\n",
            "\n",
            "Training with pace parameter λ=0.1000\n",
            "Epoch 1/5, Train Loss: 2.5745\n",
            "Epoch 5/5, Train Loss: 2.5466\n",
            "λ=0.1000: 13/66067 samples considered 'easy' (0.0%)\n",
            "\n",
            "Training with pace parameter λ=0.7071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "737xmE6LNMw6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}